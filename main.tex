% !Mode:: "TeX:UTF-8"
%!TEX program  = xelatex

\documentclass[bwprint,fontset=windows]{gmcmthesis}
% \documentclass[bwprint]{gmcmthesis}
\usepackage{longtable}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{diagbox}
\usepackage{tabularx}
\usepackage{listings}
\lstset{
	numbers=left, 
	numberstyle= \tiny, 
	keywordstyle= \color{ blue!70},%设置关键字颜色
	commentstyle= \color{red!50!green!50!blue!50}, %设置注释颜色
	frame=shadowbox, % 阴影效果
	rulesepcolor= \color{ red!20!green!20!blue!20} ,
	escapeinside=``, % 英文分号中可写入中文
	xleftmargin=2em, %距离左边界2em
	aboveskip=1em,
	framexleftmargin=2em,
	basicstyle=\ttfamily,
	columns=fullflexible,%可以自动换行
	linewidth=1\linewidth, %设置代码块与行同宽
	breaklines=true,%在单词边界处换行。
	showstringspaces=false， %去掉空格时产生的下划的空格标志, 设置为true则出现
	breakatwhitespace=ture,%可以在空格处换行
	escapechar=`%设置转义字符为反引号
}




\hypersetup{hidelinks}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage{array} % 在导言区引入 array 包
\usepackage[UTF8]{ctex} % 加载支持中文的ctex包
\usepackage{amsmath} % 加载amsmath包以支持更复杂的数学公式

%\usepackage{fontspec}  % for Consolas & Courier New
%\lstset{basicstyle=\small\fontspec{Consolas}}
%\lstset{basicstyle=\small\fontspec{Courier New}}
%\setmonofont{Consolas}
%\setcounter{tocdepth}{3}   % 调整目录深度
\usepackage{booktabs}
\usepackage{subfig}
\usepackage{colortbl}
\definecolor{color1}{rgb}{0.78,0.88,0.99}
\definecolor{color2}{rgb}{0.36,0.62,0.84}
%\definecolor{color3}{rgb}{0.8235,0.8706,0.9373}
\definecolor{color3}{rgb}{0.88,0.92,0.96}
\definecolor{color4}{rgb}{0.96,0.97,0.98}%{0.9176,0.9373,0.9686}

% 旧版的与算法有关的宏包
% 算法
% \usepackage[noend]{algpseudocode}
% \usepackage{algorithmicx,algorithm}
% \floatname{algorithm}{算法}
% \renewcommand{\algorithmicrequire}{\textbf{输入:}}
% \renewcommand{\algorithmicensure}{\textbf{输出:}}

% %\numberwithin{equation}{section}
% %\numberwithin{figure}{section}
% %\numberwithin{table}{section}
% \newcommand{\red}[1]{\textcolor{red}{#1}}
% \newcommand{\blue}[1]{\textcolor{blue}{#1}}

% 新版的算法与定理有关宏包与命令
% \usepackage[ruled,vlined]{algorithm2e}  % 支持跨页的算法宏包


%===================== 建模论文题目 ======================

\title{基于WLAN组网中网络吞吐量的精准建模}
\baominghao{\hspace{6em} 24102860070} %参赛队号
\schoolname{\hspace{5.8em} 华南师范大学}%学校名称
\membera{\hspace{6em} 曾文泉} %队员A
\memberb{\hspace{6em} 蔡镛任} %队员B
\memberc{\hspace{6em} 刘时宇} %队员C

%=======================================================


\begin{document}

 %生成标题
% \maketitle

 %填写摘要
% \begin{abstract}

% 本研究针对无线局域网（WLAN）在同频组网环境中的性能挑战，特别是同频干扰或共信道干扰（CCI）对网络性能的影响，提出了基于实测数据的特征构建和分析方法，进一步训练Stacking集成学习模型，实现了对WLAN系统吞吐量的精确预测，克服了传统仿真模型在实际部署中的局限性。


% 针对问题一，首先对\textbf{数据预处理}，选取合适的参数作为特征值进行\textbf{相关性分析}。本文发现RSSI相关参数对AP发送机会具有较高影响但特征维度较大，因此基于RSSI，进一步构建\textbf{信干噪比（SINR）}和\textbf{信号质量（Sq）}两类指标实现降维。同时，结合强弱影响排序，选取\textbf{5类特征值}（在训练集中变量名为：SINR、protocol、sq、per和eirp），构建\textbf{Stacking集成学习}模型。通过交叉验证和网格搜索，获取最优化参数。最终，2个AP测试集上的R\textsuperscript{2}为\textbf{0.98}，均方根误差仅为\textbf{1.45}；3个AP测试集上的R\textsuperscript{2}为\textbf{0.94}，均方根误差仅为\textbf{2.75}。实现了对AP发送机会的高精度预测。

% 针对问题二，考虑到MCS和NSS为分类变量，且样本比例极不均衡。结合问题一，在深入分析训练集中各参数与MCS和NSS的相关性后，发现MCS与5个变量的相关性均大于\textbf{0.5}，包括变量erip、SINR和Sq。而NSS仅与MCC有较强相关性，约为0.4。因此，本文构建了\textbf{层次预测模型}，首先预测上层目标变量MCS，将预测结果作为输入特征，进一步预测下层目标变量NSS。在进行下层预测时，考虑到NSS变量的\textbf{长尾分布特征}，提出\textbf{Kmeans聚类欠采样}和\textbf{SMOTE过采样方法}，解决样本不均衡问题。然后对模型通过交叉验证和网格搜索，获取最优化参数。最终，上层MCS测试集的Recall为\textbf{0.81}，F1分数为\textbf{0.80}；下层NSS测试集的Recall为\textbf{0.92}，F1分数为\textbf{0.92}。进一步对模型的性能评估，证实了该模型在上层特征提取和下层样本分类方面能力显著，实现了对（MCS,NSS）更优的预测结果。

% 针对问题三，首先分析对系统吞吐量有影响的变量，主要分析了随机回退机制，得到了$CW_{min}=6$ 下,2个AP下的随机回退时间期望,还分析了数据帧聚合机制对于吞吐量的影响,确定使用层次 stacking 预测模型，实现两次级联完成新特征对吞吐量的预测。对模型通过交叉验证和网格搜索，获取最优化参数。最终，该模型整体的R\textsuperscript{2}为\textbf{0.91}，平均绝对误差（MAE）为\textbf{11.9}。然后对模型进行评估，证实了该模型对WLAN吞吐量的高精度预测。

% \keywords{无线局域网; 吞吐量; 集成学习; 层次预测; 重采样}

% \end{abstract}

% \pagestyle{plain}

% %目录 不推荐加
% \maketoc

% \clearpage

% 分章节写，不用的暂时先注释
% \include{chapter1}
% \include{chapter2}
% \include{chapter3}
% \include{chapter4}
\include{chapter5}
% \include{chapter6}
% \include{chapter7}




% 下面的暂时不分章节
\section{模型的评价与改进}
\subsection{模型的评价}

\begin{enumerate}
	\item 本文中构建的模型具有一定的灵活性，由于是对已有复杂的实测数据进行降维处理后，通过实际机理过程对数据进行二次加工，构建新特征作为模型的输入，保证了模型在数据上的灵活性以及模型的准确性。
	\item 本文中构建的模型具有一定的综合性，为了避免利用单一的学习器造成的过拟合情况，利用集成学习对数据进行二次学习，利用元学习器对最终数据进行学习并预测，通过网格搜索进行集成学习器的参数优化，提升模型的精度，并同时保持模型的综合性。
\end{enumerate}

\subsection{模型的改进}

\begin{enumerate}
	\item 在问题3分析中，对于随机回退机制的回退到0碰撞概率计算不严谨，在模型优化过程中，由于时间竞争窗口相关参数的缺失，导致模型对实际情况的预测不够精准。这一缺陷影响了模型在动态环境中对数据流量的准确把握，使得无法充分考虑竞争带来的延迟和数据丢失。因此，针对这一问题，我们需要改进模型结构，集成时间竞争窗口的相关参数，以更全面地反映系统的实时状态。这将增强模型在多任务环境中的适应性和预测能力，从而提升整体性能和准确性。
	
	\item 由于RSSI采样频率的限制，模型无法进行实时检测，从而可能导致数据延迟和信息缺失。这种情况会引发模型预测的误差，特别是在快速变化的环境中，无法准确捕捉到信号强度的动态变化。因此，为了改善这一问题，我们需要提高RSSI数据的采样频率，或引入更先进的数据插值和预测技术，以确保模型能够及时获取最新的信号信息。此外，结合时序数据分析方法，可以更好地捕捉到信号强度的变化趋势，从而提升模型的准确性和鲁棒性。通过这些改进，我们将能够更精确地反映实际信号环境，提高模型在复杂场景下的应用效果。
\end{enumerate}



\newpage

\begin{thebibliography}{9}
	
	\bibitem{reference1}
	王德营, 胡威, 吴通, 等. 基于Stacking集成学习的CANDU堆通道功率预测研究[J]. 核动力工程, 2024, 45(S1): 72-77. DOI: 10.13832/j.jnpe.2024.S1.0072.
	
	\bibitem{reference2}
	杨永鹏, 刘天琦, 杨真真. 一种基于IEEE 802.11ac协议标准的帧聚合实现算法[J]. 软件导刊, 2019, 18(09): 192-195.
	
	\bibitem{reference3}
	周海兴. 竞争窗口大小对IEEE 802.11无线网络的影响[J]. 广东通信技术, 2008, (10): 40-43+48.
	
	\bibitem{reference4}
	Yeh J H, Chen J C, Lee C C. WLAN standards[J]. IEEE Potentials, 2003, 22(4): 16-22.
	
	\bibitem{reference5}
	Muhammad Asif Khan, Ridha Hamila, Nasser Ahmed Al-Emadi, Serkan Kiranyaz, Moncef Gabbouj. Real-time throughput prediction for cognitive Wi-Fi networks[J]. Journal of Network and Computer Applications, Volume 150, 2020, 102499. ISSN 1084-8045.
	
\end{thebibliography}


%参考文献   手工录入
%\begin{thebibliography}{9}%宽度9
% \bibitem{bib:one} ....
% \bibitem{bib:two} ....
%\end{thebibliography}

%采用bibtex方案
\newpage

\appendix
\section{特征预处理代码}

\noindent 在本附录中，展示了用于特征预处理的 Python 代码。

\begin{lstlisting}
		# %% 特征预处理
	import pandas as pd
	# 读取 CSV 文件
	file_path = 'training_set_3ap_loc33_nav82.csv'
	data = pd.read_csv(file_path)
	
	# 提取第一行的 pd, ed, nav 和 pkt_len
	pd_, ed, nav, pk_len = data['pd'].iloc[0], data['ed'].iloc[0], data['nav'].iloc[0], data['pkt_len'].iloc[0]
	
	# 删除无关列
	data = data.drop(['test_dur', 'pd', 'ed', 'nav', 'loc_id', 'pkt_len', 'ap_name', 'ap_mac', 'sta_mac'], axis=1)
	
	# 将 ap_id 和 sta_id 转换为整数
	data['ap_id'] = data['ap_id'].str.replace('ap_', '').astype(int)
	data['sta_id'] = data['sta_id'].str.replace('sta_', '').astype(int)
	
	# 将协议映射为对应的数字
	data['protocol'] = data['protocol'].map({'tcp': 20 + 64, 'udp': 8})
	
	# 处理特定列的数据
	columns_to_process_ap = [col for col in data.columns if col.startswith('ap_from') or col.startswith('ap_to')]
	data_ = data[columns_to_process_ap].applymap(lambda x: eval(x) if isinstance(x, str) and x.startswith('[') else [])
	
	# 去除异常值的函数
	def remove_outliers(data):
	data = pd.Series(data)
	mean = np.mean(data)
	std = np.std(data)
	lower_bound = mean - 3 * std
	upper_bound = mean + 3 * std
	return [x for x in data if x >= lower_bound and x <= upper_bound]
	
	# 对特定列去除异常值
	for col in columns_to_process_ap:
	data_[col] = data_[col].apply(remove_outliers)
	
	# 处理列表并生成统计特征
	def process_list_to_stats(df, column_name):
	if 'sum' in column_name:
	df[column_name] = df[column_name].apply(
	lambda x: math.log10(sum(math.pow(10, item) for item in x) / len(x)) if len(x) > 0 else 0
	)
	df = df.rename(columns={column_name: column_name + '_mean'})
	# 其他处理...
	return df
	
	# 对每一列应用统计特征处理
	for col in columns_to_process_ap:
	data = process_list_to_stats(data, col)
	
	# 计算噪声特征
	columns_to_process_ap_sum_noise = [col for col in data.columns if col.startswith('ap_from_ap') and 'sum' in col]
	data['ap_from_other_ap_sum_rssi_noise'] = data[columns_to_process_ap_sum_noise].applymap(
	lambda x: math.pow(10, x) if x < -10 else pow(10, -99)
	).sum(axis=1).apply(math.log10)
\end{lstlisting}


\noindent 在本附录中，展示了用于随即回退机制的 Python 代码。

\begin{lstlisting}

# %% 随即回退分析
def P(n,ap_num,CWmin):
Pn=1
for i in range(1,n):
Pn*=(ap_num-1) / (CWmin* 2**i )
Pn*=(1-(ap_num-1)/(CWmin * 2**n))
return Pn
import numpy as np
times=10 #十次
times_try = np.arange(1,times)
Pns = np.array([round(P(n, 2, 6), 10) for n in times_try])

import matplotlib.pyplot as plt
plt.rcParams['font.family'] = 'SimHei' # 选择一个支持中文的字体
plt.rcParams['axes.unicode_minus'] = False # 正确显示负号
bars = plt.bar(times_try, Pns, width=0.2)  # 设置柱子的粗度为0.5
plt.xticks(times_try)  # 将x轴下标以整数形式显示
plt.xlabel('发送次数')
plt.ylabel('概率')
plt.title('发送次数概率分布图')
plt.axhline(y=0, color='k', linestyle='--')
plt.axvline(x=0, color='k', linestyle='--')
for bar in bars:
yval = bar.get_height()
plt.text(bar.get_x() + bar.get_width()/2-0.2, yval, round(yval, 2), va='bottom')
plt.show()

CWmin=6
slotTime=9 #9us
CWmax=2100 * slotTime
times_cost=np.array([min(CWmax ,(CWmin-1)*slotTime/2 *(2**n-1) ) for n in range(1,times)])
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(times_try, times_cost, color='skyblue', marker='o')
plt.xlabel('发送次数')
plt.ylabel('时间成本/μs')
plt.title('发送次数与时间成本的关系')
plt.axhline(y=0, color='k', linestyle='--')
plt.axvline(x=0, color='k', linestyle='--')
plt.grid(False)
plt.show()


plt.figure(figsize=(10, 6))
plt.plot(times_try, times_cost * Pns, color='skyblue', marker='o', linestyle='-', linewidth=2)  # 设置线的颜色、标记、样式和宽度
plt.xlabel('发送次数')
plt.ylabel('时间成本与概率的乘积')
plt.title('发送次数和时间成本期望的关系')
plt.axhline(y=0, color='k', linestyle='--')
plt.axvline(x=0, color='k', linestyle='--')
plt.grid(False)
plt.show()
np.sum(times_cost * Pns)
\end{lstlisting}


\section{问题一代码}

\noindent 在本附录中，展示了2AP的预测模型代码。

\begin{lstlisting}
# 定义文件夹路径
folder_path = Path(r'C:\Users\Shawn\Desktop\shumo\2024年中国研究生数学建模竞赛赛题\B题\2AP_cleaned_data')

# 初始化一个空的列表用于存储所有数据
dataframes = []

# 遍历文件夹中的所有CSV文件
for file_path in folder_path.glob('*.csv'):
data = pd.read_csv(file_path)
dataframes.append(data)

# 合并所有 DataFrame，保留所有列
combined_data = pd.concat(dataframes, ignore_index=True, sort=False)

# 保存到新的CSV文件
# combined_data.to_csv(r'C:\Users\Shawn\Desktop\shumo\2024年中国研究生数学建模竞赛赛题\B题\combined_ap2.csv', index=False)

# 删去不需要的列
columns_to_remove = [
'predict throughput', 'error%', 'bss_id', 'ap_name','sta_id','test_id',
'ap_from_other_ap_sum_rssi_noise', 'ap_from_other_ap_max_rssi_noise',
'ap_from_other_ap_mean_rssi_noise', 'sta_id', 'sta_to_ap_0_sum_ant_rssi_mean',
'sta_to_ap_0_max_ant_rssi_free_rate', 'sta_to_ap_0_mean_ant_rssi_free_rate',
'sta_to_ap_1_sum_ant_rssi_mean', 'sta_to_ap_1_max_ant_rssi_free_rate',

'sta_to_ap_1_mean_ant_rssi_free_rate', 'sta_from_ap_0_sum_ant_rssi_mean',
'sta_from_ap_0_max_ant_rssi_free_rate', 'sta_from_ap_0_mean_ant_rssi_free_rate',
'sta_from_ap_1_sum_ant_rssi_mean', 'sta_from_ap_1_max_ant_rssi_free_rate',
'sta_from_ap_1_mean_ant_rssi_free_rate', 'sta_from_other_sta_sum', 'num_ampdu',
'ppdu_dur', 'other_air_time', 'sta_from_ap_signal', 'sta_from_ap_noise',

'sta_to_ap_signal', 'sta_to_ap_noise', 'sta_from_ap_signal_max_free_rate',
'sta_from_ap_noise_max_free_rate', 'sta_to_ap_signal_max_free_rate',
'sta_to_ap_noise_max_free_rate', 'sta_from_ap_signal_mean_free_rate',
'sta_from_ap_noise_mean_free_rate', 'sta_to_ap_signal_mean_free_rate',
'sta_to_ap_noise_mean_free_rate',
]

combined_data = combined_data.drop(columns=columns_to_remove, errors='ignore')

# 计算相关系数矩阵
correlations = combined_data.corr()

# 获取 'seq_time' 与其他变量的相关性并排序
seq_time_correlations = correlations['seq_time'].sort_values(ascending=False)

print("影响强弱排序:")
print(seq_time_correlations[1:])


# 选择相关性最强的特征
top_correlated_features = seq_time_correlations.index[1:].tolist()

# 去掉 'throughput' 列
top_correlated_features = [feature for feature in top_correlated_features if feature != 'throughput']


# # 选择相关性最强的前9个变量（去掉相关性最强的一个变量）
# top_correlated_features = seq_time_correlations.index[1:].tolist()

# 选择相关性最强的特征作为模型输入
X = combined_data[top_correlated_features]
y = combined_data['seq_time']

# 分割训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 定义数据预处理器，数值特征标准化
numeric_transformer = StandardScaler()

# 创建预处理步骤
preprocessor = ColumnTransformer(
transformers=[
('num', numeric_transformer, top_correlated_features)
])

# 定义多个模型进行对比
models = {
	'Gradient Boosting': GradientBoostingRegressor(random_state=42),
	'Random Forest': RandomForestRegressor(random_state=42),
	'Support Vector Regressor': SVR(),
	'Linear Regression': LinearRegression()
}

# 初始化一个字典存储模型的性能指标
results = {}

# 遍历每个模型，训练并评估它们的性能
for model_name, model in models.items():
# 创建pipeline
pipeline = Pipeline([
('preprocessor', preprocessor),  # 数据预处理
('regressor', model)  # 使用模型
])

# 训练模型
pipeline.fit(X_train, y_train)

# 预测
y_pred = pipeline.predict(X_test)

# 计算误差和评价指标
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# 将结果存入字典
results[model_name] = {
	'MSE': mse,
	'RMSE': rmse,
	'MAE': mae,
	'R²': r2
}

# 将结果输出为DataFrame
results_df = pd.DataFrame(results).T
print(results_df)

# 使用Stacking集成学习模型
stacking_regressor = StackingRegressor(
estimators=[
('gb', GradientBoostingRegressor(random_state=42)),
('rf', RandomForestRegressor(random_state=42)),
('svr', SVR())
],
final_estimator=LinearRegression()
)

# 创建Stacking模型的pipeline
stacking_pipeline = Pipeline([
('preprocessor', preprocessor),  # 数据预处理
('stacking_regressor', stacking_regressor)  # 集成模型
])

# 训练Stacking模型
stacking_pipeline.fit(X_train, y_train)

# 预测
y_pred_stack = stacking_pipeline.predict(X_test)

# 计算Stacking模型的性能指标
mse_stack = mean_squared_error(y_test, y_pred_stack)
rmse_stack = np.sqrt(mse_stack)
mae_stack = mean_absolute_error(y_test, y_pred_stack)
r2_stack = r2_score(y_test, y_pred_stack)

# 输出Stacking模型的评价指标
print("\nStacking模型性能：")
print(f"MSE: {mse_stack}")
print(f"RMSE: {rmse_stack}")
print(f"MAE: {mae_stack}")
print(f"R²: {r2_stack}")

# 将Stacking模型的性能添加到结果中
results['Stacking Model'] = {
	'MSE': mse_stack,
	'RMSE': rmse_stack,
	'MAE': mae_stack,
	'R²': r2_stack
}

# 更新结果展示
results_df = pd.DataFrame(results).T
print(results_df)



# 参数调整和优化
param_grid = {
	'stacking_regressor__gb__n_estimators': [100, 200],
	'stacking_regressor__gb__learning_rate': [0.05, 0.3],
	'stacking_regressor__rf__n_estimators': [100, 200],
	'stacking_regressor__rf__max_depth': [5, 10],
	'stacking_regressor__svr__C': [0.1, 1, 10],
	'stacking_regressor__svr__kernel': ['linear', 'rbf']
}

grid_search = GridSearchCV(estimator=stacking_pipeline, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)
grid_search.fit(X_train, y_train)
best_params = grid_search.best_params_
best_score = -grid_search.best_score_

print("\nBest Grid Search Score and Parameters:")
print(f"Best Score: {best_score}")
print(f"Best Parameters: {best_params}")

# 最佳参数模型评估
best_stacking_model = grid_search.best_estimator_
y_pred_best_stack = best_stacking_model.predict(X_test)
mse_best_stack = mean_squared_error(y_test, y_pred_best_stack)
rmse_best_stack = np.sqrt(mse_best_stack)
mae_best_stack = mean_absolute_error(y_test, y_pred_best_stack)
r2_best_stack = r2_score(y_test, y_pred_best_stack)

print("\nStacking最佳参数模型性能：")
print("\nOptimized Stacking Model Performance:")
print(f"MSE: {mse_best_stack}")
print(f"RMSE: {rmse_best_stack}")
print(f"MAE: {mae_best_stack}")
print(f"R²: {r2_best_stack}")











import joblib

# 将模型保存到磁盘
model_filename = '2AP_stacking_model.pkl'
joblib.dump(best_stacking_model, model_filename)

print(f"模型已保存至 {model_filename}")

\end{lstlisting}





\noindent 在本附录中，展示了3AP的预测模型代码。

\begin{lstlisting}
	# 定义文件夹路径
	folder_path = Path(r'C:\Users\Shawn\Desktop\shumo\2024年中国研究生数学建模竞赛赛题\B题\3AP_cleaned_data')
	
	# 初始化一个空的列表用于存储所有数据
	dataframes = []
	
	# 遍历文件夹中的所有CSV文件
	for file_path in folder_path.glob('*.csv'):
	data = pd.read_csv(file_path)
	dataframes.append(data)
	
	# 合并所有 DataFrame，保留所有列
	combined_data = pd.concat(dataframes, ignore_index=True, sort=False)
	
	# 添加 SINR 列
	combined_data['SINR'] = combined_data['sta_from_ap_signal'] - combined_data['sta_from_ap_noise']
	
	
	# 将结果保存到CSV文件
	combined_data.to_csv(r'C:\Users\Shawn\Desktop\shumo\2024年中国研究生数学建模竞赛赛题\B题\combined_ap3.csv', index=False)
	
	# 删去不需要的列
	columns_to_remove = [
	'predict throughput', 'error%', 'bss_id', 'ap_name','sta_id','test_id',
	'ap_from_other_ap_sum_rssi_noise', 'ap_from_other_ap_max_rssi_noise',
	'ap_from_other_ap_mean_rssi_noise', 'sta_id', 'sta_to_ap_0_sum_ant_rssi_mean',
	'sta_to_ap_0_max_ant_rssi_free_rate', 'sta_to_ap_0_mean_ant_rssi_free_rate',
	'sta_to_ap_1_sum_ant_rssi_mean', 'sta_to_ap_1_max_ant_rssi_free_rate',
	
	'sta_to_ap_1_mean_ant_rssi_free_rate', 'sta_from_ap_0_sum_ant_rssi_mean',
	'sta_from_ap_0_max_ant_rssi_free_rate', 'sta_from_ap_0_mean_ant_rssi_free_rate',
	'sta_from_ap_1_sum_ant_rssi_mean', 'sta_from_ap_1_max_ant_rssi_free_rate',
	'sta_from_ap_1_mean_ant_rssi_free_rate', 'sta_from_other_sta_sum', 'num_ampdu',
	'ppdu_dur', 'other_air_time', 'sta_from_ap_signal', 'sta_from_ap_noise',
	
	'sta_to_ap_signal', 'sta_to_ap_noise', 'sta_from_ap_signal_max_free_rate',
	'sta_from_ap_noise_max_free_rate', 'sta_to_ap_signal_max_free_rate',
	'sta_to_ap_noise_max_free_rate', 'sta_from_ap_signal_mean_free_rate',
	'sta_from_ap_noise_mean_free_rate', 'sta_to_ap_signal_mean_free_rate',
	'sta_to_ap_noise_mean_free_rate',
	
	'sta_to_ap_2_mean_ant_rssi_free_rate', 'sta_to_ap_2_max_ant_rssi_free_rate',
	'sta_to_ap_2_sum_ant_rssi_mean',           
	'sta_from_ap_2_max_ant_rssi_free_rate',    
	'sta_from_ap_2_sum_ant_rssi_mean',         
	'sta_from_ap_2_mean_ant_rssi_free_rate',
	]
	
	combined_data = combined_data.drop(columns=columns_to_remove, errors='ignore')
	
	# 计算相关系数矩阵
	correlations = combined_data.corr()
	
	# 获取 'seq_time' 与其他变量的相关性并排序
	seq_time_correlations = correlations['seq_time'].sort_values(ascending=False)
	
	print("影响强弱排序:")
	print(seq_time_correlations[1:])
	
	# 选择相关性最强的特征
	top_correlated_features = seq_time_correlations.index[1:].tolist()
	
	# 去掉 'throughput' 列
	top_correlated_features = [feature for feature in top_correlated_features if feature != 'throughput']
	
	
	# # 选择相关性最强的前9个变量（去掉相关性最强的一个变量）
	# top_correlated_features = seq_time_correlations.index[1:].tolist()
	
	# 选择相关性最强的特征作为模型输入
	X = combined_data[top_correlated_features]
	y = combined_data['seq_time']
	
	# 分割训练集和测试集
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
	
	# 定义数据预处理器，数值特征标准化
	numeric_transformer = StandardScaler()
	
	# 创建预处理步骤
	preprocessor = ColumnTransformer(
	transformers=[
	('num', numeric_transformer, top_correlated_features)
	])
	
	# 定义多个模型进行对比
	models = {
		'Gradient Boosting': GradientBoostingRegressor(random_state=42),
		'Random Forest': RandomForestRegressor(random_state=42),
		'Support Vector Regressor': SVR(),
		'Linear Regression': LinearRegression()
	}
	
	# 初始化一个字典存储模型的性能指标
	results = {}
	
	# 遍历每个模型，训练并评估它们的性能
	for model_name, model in models.items():
	# 创建pipeline
	pipeline = Pipeline([
	('preprocessor', preprocessor),  # 数据预处理
	('regressor', model)  # 使用模型
	])
	
	# 训练模型
	pipeline.fit(X_train, y_train)
	
	# 预测
	y_pred = pipeline.predict(X_test)
	
	# 计算误差和评价指标
	mse = mean_squared_error(y_test, y_pred)
	rmse = np.sqrt(mse)
	mae = mean_absolute_error(y_test, y_pred)
	r2 = r2_score(y_test, y_pred)
	
	# 将结果存入字典
	results[model_name] = {
		'MSE': mse,
		'RMSE': rmse,
		'MAE': mae,
		'R²': r2
	}
	
	# 将结果输出为DataFrame
	results_df = pd.DataFrame(results).T
	print(results_df)
	
	# 使用Stacking集成学习模型
	stacking_regressor = StackingRegressor(
	estimators=[
	('gb', GradientBoostingRegressor(random_state=42)),
	('rf', RandomForestRegressor(random_state=42)),
	('svr', SVR())
	],
	final_estimator=LinearRegression()
	)
	
	# 创建Stacking模型的pipeline
	stacking_pipeline = Pipeline([
	('preprocessor', preprocessor),  # 数据预处理
	('stacking_regressor', stacking_regressor)  # 集成模型
	])
	
	# 训练Stacking模型
	stacking_pipeline.fit(X_train, y_train)
	
	# 预测
	y_pred_stack = stacking_pipeline.predict(X_test)
	
	# 计算Stacking模型的性能指标
	mse_stack = mean_squared_error(y_test, y_pred_stack)
	rmse_stack = np.sqrt(mse_stack)
	mae_stack = mean_absolute_error(y_test, y_pred_stack)
	r2_stack = r2_score(y_test, y_pred_stack)
	
	# 输出Stacking模型的评价指标
	print("\nStacking模型性能：")
	print(f"MSE: {mse_stack}")
	print(f"RMSE: {rmse_stack}")
	print(f"MAE: {mae_stack}")
	print(f"R²: {r2_stack}")
	
	# 将Stacking模型的性能添加到结果中
	results['Stacking Model'] = {
		'MSE': mse_stack,
		'RMSE': rmse_stack,
		'MAE': mae_stack,
		'R²': r2_stack
	}
	
	# 更新结果展示
	results_df = pd.DataFrame(results).T
	print(results_df)
	
	
	# 参数调整和优化
	param_grid = {
		'stacking_regressor__gb__n_estimators': [100, 200],
		'stacking_regressor__gb__learning_rate': [0.05, 0.3],
		'stacking_regressor__rf__n_estimators': [100, 200],
		'stacking_regressor__rf__max_depth': [5, 10],
		'stacking_regressor__svr__C': [0.1, 1, 10],
		'stacking_regressor__svr__kernel': ['linear', 'rbf']
	}
	
	grid_search = GridSearchCV(estimator=stacking_pipeline, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)
	grid_search.fit(X_train, y_train)
	best_params = grid_search.best_params_
	best_score = -grid_search.best_score_
	
	print("\nBest Grid Search Score and Parameters:")
	print(f"Best Score: {best_score}")
	print(f"Best Parameters: {best_params}")
	
	# 最佳参数模型评估
	best_stacking_model = grid_search.best_estimator_
	y_pred_best_stack = best_stacking_model.predict(X_test)
	mse_best_stack = mean_squared_error(y_test, y_pred_best_stack)
	rmse_best_stack = np.sqrt(mse_best_stack)
	mae_best_stack = mean_absolute_error(y_test, y_pred_best_stack)
	r2_best_stack = r2_score(y_test, y_pred_best_stack)
	
	print("\nStacking最佳参数模型性能：")
	print("\nOptimized Stacking Model Performance:")
	print(f"MSE: {mse_best_stack}")
	print(f"RMSE: {rmse_best_stack}")
	print(f"MAE: {mae_best_stack}")
	print(f"R²: {r2_best_stack}")
	
	
	
	
	
	# 可视化Stacking模型的预测结果
	plt.figure(figsize=(10, 6))
	
	# 1. 拟合线图（预测值 vs 实际值）
	plt.subplot(1, 3, 1)  # 调整为 1 行 3 列的图
	plt.scatter(y_test, y_pred_stack, alpha=0.5, color='blue')
	plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', lw=2)
	plt.xlabel('实际值')
	plt.ylabel('预测值')
	plt.title('Stacking模型预测值与实际值的拟合图')
	plt.grid(True)
	
	# 2. 残差图（残差 vs 预测值）
	plt.subplot(1, 3, 2)
	residuals = y_test - y_pred_stack
	plt.scatter(y_pred_stack, residuals, alpha=0.5, color='green')
	plt.axhline(0, color='red', lw=2)
	plt.xlabel('预测值')
	plt.ylabel('残差')
	plt.title('Stacking模型残差图')
	plt.grid(True)
	
	# 3. 预测值 vs 实际值的散点图
	plt.subplot(1, 3, 3)
	plt.scatter(range(len(y_test)), y_test, alpha=0.5, color='blue', label='实际值')
	plt.scatter(range(len(y_pred_stack)), y_pred_stack, alpha=0.5, color='orange', label='预测值')
	plt.xlabel('样本索引')
	plt.ylabel('seq_time')
	plt.title('Stacking模型预测值与实际值的散点图')
	plt.legend()
	plt.grid(True)
	
	# 调整布局并展示图形
	plt.tight_layout()
	plt.show()
	
	# 误差分布图
	plt.figure(figsize=(8, 5))
	sns.histplot(residuals, bins=30, kde=True, color='purple')
	plt.xlabel('残差')
	plt.ylabel('频率')
	plt.title('(b) 3AP')
	plt.grid(True)
	plt.show()
	
	import joblib
	
	# 将模型保存到磁盘
	model_filename = '3AP_stacking_model.pkl'
	joblib.dump(best_stacking_model, model_filename)
	
	print(f"模型已保存至 {model_filename}")
	
\end{lstlisting}




\section{问题二代码}
\noindent 在本附录中，展示了层次预测模型代码。

\begin{lstlisting}

	
	
	# 定义文件夹路径
	folder_path1 = Path(r'C:\Users\Shawn\Desktop\shumo\2024年中国研究生数学建模竞赛赛题\B题\2AP_cleaned_data')
	
	# 初始化一个空的列表用于存储所有数据
	dataframes1 = []
	
	# 遍历文件夹中的所有CSV文件
	for file_path1 in folder_path1.glob('*.csv'):
	data = pd.read_csv(file_path1)
	dataframes1.append(data)
	
	# 合并所有 DataFrame，保留所有列
	combined_data1 = pd.concat(dataframes1, ignore_index=True, sort=False)
	
	# 删去不需要的列
	columns_to_remove = [
	'predict throughput', 'error%', 'bss_id', 'ap_name','sta_id','test_id',
	'ap_from_other_ap_sum_rssi_noise', 'ap_from_other_ap_max_rssi_noise',
	'ap_from_other_ap_mean_rssi_noise', 'sta_id', 'sta_to_ap_0_sum_ant_rssi_mean',
	'sta_to_ap_0_max_ant_rssi_free_rate', 'sta_to_ap_0_mean_ant_rssi_free_rate',
	'sta_to_ap_1_sum_ant_rssi_mean', 'sta_to_ap_1_max_ant_rssi_free_rate',
	
	'sta_to_ap_1_mean_ant_rssi_free_rate', 'sta_from_ap_0_sum_ant_rssi_mean',
	'sta_from_ap_0_max_ant_rssi_free_rate', 'sta_from_ap_0_mean_ant_rssi_free_rate',
	'sta_from_ap_1_sum_ant_rssi_mean', 'sta_from_ap_1_max_ant_rssi_free_rate',
	'sta_from_ap_1_mean_ant_rssi_free_rate', 'sta_from_other_sta_sum', 'num_ampdu',
	'ppdu_dur', 'other_air_time', 'sta_from_ap_signal', 'sta_from_ap_noise',
	
	'sta_to_ap_signal', 'sta_to_ap_noise', 'sta_from_ap_signal_max_free_rate',
	'sta_from_ap_noise_max_free_rate', 'sta_to_ap_signal_max_free_rate',
	'sta_to_ap_noise_max_free_rate', 'sta_from_ap_signal_mean_free_rate',
	'sta_from_ap_noise_mean_free_rate', 'sta_to_ap_signal_mean_free_rate',
	'sta_to_ap_noise_mean_free_rate',
	
	'seq_time','throughput',
	]
	
	combined_data1 = combined_data1.drop(columns=columns_to_remove, errors='ignore')
	
	# 定义文件夹路径
	folder_path2 = Path(r'C:\Users\Shawn\Desktop\shumo\2024年中国研究生数学建模竞赛赛题\B题\3AP_cleaned_data')
	
	# 初始化一个空的列表用于存储所有数据
	dataframes2 = []
	
	# 遍历文件夹中的所有CSV文件
	for file_path2 in folder_path2.glob('*.csv'):
	data = pd.read_csv(file_path2)
	dataframes2.append(data)
	
	# 合并所有 DataFrame，保留所有列
	combined_data2 = pd.concat(dataframes2, ignore_index=True, sort=False)
	
	# 添加 SINR 列
	combined_data2['SINR'] = combined_data2['sta_from_ap_signal'] - combined_data2['sta_from_ap_noise']
	
	
	# 删去不需要的列
	columns_to_remove2 = [
	'predict throughput', 'error%', 'bss_id', 'ap_name','sta_id','test_id',
	'ap_from_other_ap_sum_rssi_noise', 'ap_from_other_ap_max_rssi_noise',
	'ap_from_other_ap_mean_rssi_noise', 'sta_id', 'sta_to_ap_0_sum_ant_rssi_mean',
	'sta_to_ap_0_max_ant_rssi_free_rate', 'sta_to_ap_0_mean_ant_rssi_free_rate',
	'sta_to_ap_1_sum_ant_rssi_mean', 'sta_to_ap_1_max_ant_rssi_free_rate',
	
	'sta_to_ap_1_mean_ant_rssi_free_rate', 'sta_from_ap_0_sum_ant_rssi_mean',
	'sta_from_ap_0_max_ant_rssi_free_rate', 'sta_from_ap_0_mean_ant_rssi_free_rate',
	'sta_from_ap_1_sum_ant_rssi_mean', 'sta_from_ap_1_max_ant_rssi_free_rate',
	'sta_from_ap_1_mean_ant_rssi_free_rate', 'sta_from_other_sta_sum', 'num_ampdu',
	'ppdu_dur', 'other_air_time', 'sta_from_ap_signal', 'sta_from_ap_noise',
	
	'sta_to_ap_signal', 'sta_to_ap_noise', 'sta_from_ap_signal_max_free_rate',
	'sta_from_ap_noise_max_free_rate', 'sta_to_ap_signal_max_free_rate',
	'sta_to_ap_noise_max_free_rate', 'sta_from_ap_signal_mean_free_rate',
	'sta_from_ap_noise_mean_free_rate', 'sta_to_ap_signal_mean_free_rate',
	'sta_to_ap_noise_mean_free_rate',
	
	'sta_to_ap_2_mean_ant_rssi_free_rate', 'sta_to_ap_2_max_ant_rssi_free_rate',
	'sta_to_ap_2_sum_ant_rssi_mean',           
	'sta_from_ap_2_max_ant_rssi_free_rate',    
	'sta_from_ap_2_sum_ant_rssi_mean',         
	'sta_from_ap_2_mean_ant_rssi_free_rate',
	
	'seq_time','throughput',
	]
	
	combined_data2 = combined_data2.drop(columns=columns_to_remove2, errors='ignore')
	
	# 合并AP2和AP3
	combined_data = pd.concat([combined_data1, combined_data2], ignore_index=True)
	combined_data = combined_data.dropna(axis=1, how='any')
	
	# 删除 nss 列值为 0 的行
	combined_data = combined_data[combined_data['nss'] != 0]
	
	# 计算 nss 列中值为 2 的数量
	count_nss_2 = (combined_data['nss'] == 2).sum()
	
	# 计算 nss 列的总数
	total_count = combined_data.shape[0]
	
	# 计算占比
	percentage_nss_2 = count_nss_2 / total_count * 100
	
	# 打印结果
	print(f"nss 列值为 2 的占比: {percentage_nss_2:.2f}%")
	
	
	
	# 计算相关系数矩阵
	correlations = combined_data.corr()
	
	# 获取 'nss' 与其他变量的相关性并排序
	MCS_correlations = correlations['nss'].sort_values(ascending=False)
	
	print("'nss' 与其他变量的影响强弱排序:")
	print(MCS_correlations[1:])
	
	# 选择相关性最强的特征
	top_correlated_features = MCS_correlations.index[1:].tolist()
	
	# 去掉 'throughput' 列
	top_correlated_features = [feature for feature in top_correlated_features if feature != 'throughput']
	
	# 选择相关性最强的特征作为模型输入
	X = combined_data[top_correlated_features]
	y = combined_data['nss']
	
	## 聚类欠采样：使用K-means对多数类样本进行聚类，然后从每个聚类中选择代表性样本
	# 选择nss值为2的多数类样本
	majority_class = combined_data[combined_data['nss'] == 2]
	minority_class = combined_data[combined_data['nss'] == 1]
	
	# 设定K-means聚类的簇数
	num_clusters = min(11, len(majority_class))  # 根据多数类样本的数量调整簇数
	
	# 对多数类样本进行K-means聚类
	kmeans = KMeans(n_clusters=num_clusters, random_state=42)
	majority_class['cluster'] = kmeans.fit_predict(majority_class.drop(columns='nss'))
	
	# 从每个簇中选择一个样本
	representative_samples = majority_class.groupby('cluster').apply(lambda x: x.sample(n=5)).reset_index(drop=True)
	
	
	# 合并代表性样本与少数类样本
	resampled_data = pd.concat([representative_samples, minority_class], ignore_index=True)
	
	# 删去 cluster 列
	resampled_data = resampled_data.drop(columns='cluster')
	
	# 特征和标签
	X = resampled_data.drop(columns='nss')
	y = resampled_data['nss']
	
	
	
	
	# 分割训练集和测试集
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
	
	# 定义数据预处理器，数值特征标准化
	numeric_transformer = StandardScaler()
	
	# 创建预处理步骤
	preprocessor = ColumnTransformer(
	transformers=[
	('num', numeric_transformer, top_correlated_features)
	])
	
	# 使用 SMOTE 进行过采样
	smote = SMOTE(k_neighbors=10,random_state=42)
	X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)
	
	# 定义多个分类模型进行对比
	models = {
		'Gradient Boosting': GradientBoostingClassifier(random_state=42),
		'Random Forest': RandomForestClassifier(random_state=42, class_weight='balanced'),
		'Support Vector Classifier': SVC(class_weight='balanced', probability=True),
		'Logistic Regression': LogisticRegression(class_weight='balanced')
	}
	
	# 初始化一个字典存储模型的性能指标
	results = {}
	
	# 遍历每个模型，训练并评估它们的性能
	for model_name, model in models.items():
	# 创建pipeline
	pipeline = Pipeline([
	('preprocessor', preprocessor),  # 数据预处理
	('classifier', model)  # 使用模型
	])
	
	# 训练模型
	pipeline.fit(X_train_resampled, y_train_resampled)
	
	# 预测
	y_pred = pipeline.predict(X_test)
	
	# 计算评价指标
	accuracy = accuracy_score(y_test, y_pred)
	f1 = f1_score(y_test, y_pred, average='weighted')
	precision = precision_score(y_test, y_pred, average='weighted')
	recall = recall_score(y_test, y_pred, average='weighted')
	
	# 将结果存入字典
	results[model_name] = {
		'Accuracy': accuracy,
		'F1 Score': f1,
		'Precision': precision,
		'Recall': recall
	}
	
	# 将结果输出为DataFrame
	results_df = pd.DataFrame(results).T
	print(results_df)
	
	# 使用Stacking集成学习模型
	stacking_classifier = StackingClassifier(
	estimators=[
	('gb', GradientBoostingClassifier(random_state=42)),
	('rf', RandomForestClassifier(random_state=42, class_weight='balanced')),
	('svc', SVC(probability=True, class_weight='balanced'))
	],
	final_estimator=LogisticRegression(class_weight='balanced')
	)
	
	# 创建Stacking模型的pipeline
	stacking_pipeline = Pipeline([
	('preprocessor', preprocessor),  # 数据预处理
	('stacking_classifier', stacking_classifier)  # 集成模型
	])
	
	# 训练Stacking模型
	stacking_pipeline.fit(X_train_resampled, y_train_resampled)
	
	# 预测
	y_pred_stack = stacking_pipeline.predict(X_test)
	
	# 计算Stacking模型的性能指标
	accuracy_stack = accuracy_score(y_test, y_pred_stack)
	f1_stack = f1_score(y_test, y_pred_stack, average='weighted')
	precision_stack = precision_score(y_test, y_pred_stack, average='weighted')
	recall_stack = recall_score(y_test, y_pred_stack, average='weighted')
	
	# 输出Stacking模型的评价指标
	print("\nStacking模型性能：")
	print(f"Accuracy: {accuracy_stack}")
	print(f"F1 Score: {f1_stack}")
	print(f"Precision: {precision_stack}")
	print(f"Recall: {recall_stack}")
	
	# 将Stacking模型的性能添加到结果中
	results['Stacking Model'] = {
		'Accuracy': accuracy_stack,
		'F1 Score': f1_stack,
		'Precision': precision_stack,
		'Recall': recall_stack
	}
	
	# 更新结果展示
	results_df = pd.DataFrame(results).T
	print(results_df)
	
	
	# 定义Stacking模型的参数网格
	param_grid_stacking = {
		'stacking_classifier__gb__n_estimators': [100, 200],
		'stacking_classifier__gb__learning_rate': [0.01, 0.1],
		'stacking_classifier__rf__n_estimators': [100, 200],
		'stacking_classifier__rf__max_features': ['auto', 'sqrt'],
		'stacking_classifier__svc__C': [1, 10],
		'stacking_classifier__final_estimator__C': [0.1, 1, 10],
		'stacking_classifier__final_estimator__penalty': ['l1', 'l2']
	}
	
	# 配置GridSearchCV
	grid_search_stacking = GridSearchCV(
	estimator=stacking_pipeline,
	param_grid=param_grid_stacking,
	scoring='accuracy',
	cv=3,
	verbose=1,
	n_jobs=-1
	)
	
	# 执行网格搜索
	grid_search_stacking.fit(X_train, y_train)
	
	# 获取最佳参数和评分
	print("Stacking模型最佳参数和评分：", grid_search_stacking.best_params_, grid_search_stacking.best_score_)
	
	# 更新Stacking模型在测试集上的性能评估
	y_pred_stack_best = grid_search_stacking.predict(X_test)
	accuracy_stack_best = accuracy_score(y_test, y_pred_stack_best)
	f1_stack_best = f1_score(y_test, y_pred_stack_best, average='weighted')
	precision_stack_best = precision_score(y_test, y_pred_stack_best, average='weighted')
	recall_stack_best = recall_score(y_test, y_pred_stack_best, average='weighted')
	
	# 输出优化后的Stacking模型性能
	print("\n优化后的Stacking模型性能：")
	print(f"Accuracy: {accuracy_stack_best}")
	print(f"F1 Score: {f1_stack_best}")
	print(f"Precision: {precision_stack_best}")
	print(f"Recall: {recall_stack_best}")
	
	# 将优化后的性能添加到结果中
	results['Optimized Stacking Model'] = {
		'Accuracy': accuracy_stack_best,
		'F1 Score': f1_stack_best,
		'Precision': precision_stack_best,
		'Recall': recall_stack_best
	}
	
	# 更新结果DataFrame并展示
	results_df = pd.DataFrame(results).T
	print(results_df)
	
	
	
	
	
	
	
	
	
	
	

	
	# 保存最优的集成模型
	joblib.dump(grid_search_stacking.best_estimator_, 'q2_nss_stacking_model.pkl')
	print("最优的集成模型已保存为 'q2_nss_stacking_model.pkl'")
\end{lstlisting}



\section{问题三代码}


\begin{lstlisting}
	
# 定义文件夹路径
folder_path1 = Path(r'C:\Users\Shawn\Desktop\shumo\2024年中国研究生数学建模竞赛赛题\B题\2AP_cleaned_data')

# 初始化一个空的列表用于存储所有数据
dataframes1 = []

# 遍历文件夹中的所有CSV文件
for file_path1 in folder_path1.glob('*.csv'):
data = pd.read_csv(file_path1)
dataframes1.append(data)

# 合并所有 DataFrame，保留所有列
combined_data1 = pd.concat(dataframes1, ignore_index=True, sort=False)

# 删去不需要的列
columns_to_remove = [
'predict throughput', 'error%', 'bss_id', 'ap_name','test_id','sta_id',#'ap_id',
# 'ap_from_other_ap_sum_rssi_noise', 'ap_from_other_ap_max_rssi_noise',
# 'ap_from_other_ap_mean_rssi_noise', 
'sta_id', 'sta_to_ap_0_sum_ant_rssi_mean',
'sta_to_ap_0_max_ant_rssi_free_rate', 'sta_to_ap_0_mean_ant_rssi_free_rate',
'sta_to_ap_1_sum_ant_rssi_mean', 'sta_to_ap_1_max_ant_rssi_free_rate',

'sta_to_ap_1_mean_ant_rssi_free_rate', 'sta_from_ap_0_sum_ant_rssi_mean',
'sta_from_ap_0_max_ant_rssi_free_rate', 'sta_from_ap_0_mean_ant_rssi_free_rate',
'sta_from_ap_1_sum_ant_rssi_mean', 'sta_from_ap_1_max_ant_rssi_free_rate',
'sta_from_ap_1_mean_ant_rssi_free_rate', 'sta_from_other_sta_sum', 'num_ampdu',
'ppdu_dur', 'other_air_time', 'sta_from_ap_signal', 'sta_from_ap_noise',

'sta_to_ap_signal', 'sta_to_ap_noise', 'sta_from_ap_signal_max_free_rate',
'sta_from_ap_noise_max_free_rate', 'sta_to_ap_signal_max_free_rate',
'sta_to_ap_noise_max_free_rate', 'sta_from_ap_signal_mean_free_rate',
'sta_from_ap_noise_mean_free_rate', 'sta_to_ap_signal_mean_free_rate',
'sta_to_ap_noise_mean_free_rate',
'seq_time',
]

combined_data1 = combined_data1.drop(columns=columns_to_remove, errors='ignore')


# 定义文件夹路径
folder_path2 = Path(r'C:\Users\Shawn\Desktop\shumo\2024年中国研究生数学建模竞赛赛题\B题\3AP_cleaned_data')

# 初始化一个空的列表用于存储所有数据
dataframes2 = []

# 遍历文件夹中的所有CSV文件
for file_path2 in folder_path2.glob('*.csv'):
data = pd.read_csv(file_path2)
dataframes2.append(data)

# 合并所有 DataFrame，保留所有列
combined_data2 = pd.concat(dataframes2, ignore_index=True, sort=False)

# 添加 SINR 列
combined_data2['SINR'] = combined_data2['sta_from_ap_signal'] - combined_data2['sta_from_ap_noise']


# 删去不需要的列
columns_to_remove2 = [
'predict throughput', 'error%', 'bss_id', 'ap_name','test_id','sta_id',#'ap_id',
# 'ap_from_other_ap_sum_rssi_noise', 'ap_from_other_ap_max_rssi_noise',
# 'ap_from_other_ap_mean_rssi_noise', 
'sta_id', 'sta_to_ap_0_sum_ant_rssi_mean',
'sta_to_ap_0_max_ant_rssi_free_rate', 'sta_to_ap_0_mean_ant_rssi_free_rate',
'sta_to_ap_1_sum_ant_rssi_mean', 'sta_to_ap_1_max_ant_rssi_free_rate',

'sta_to_ap_1_mean_ant_rssi_free_rate', 'sta_from_ap_0_sum_ant_rssi_mean',
'sta_from_ap_0_max_ant_rssi_free_rate', 'sta_from_ap_0_mean_ant_rssi_free_rate',
'sta_from_ap_1_sum_ant_rssi_mean', 'sta_from_ap_1_max_ant_rssi_free_rate',
'sta_from_ap_1_mean_ant_rssi_free_rate', 'sta_from_other_sta_sum', 'num_ampdu',
'ppdu_dur', 'other_air_time', 'sta_from_ap_signal', 'sta_from_ap_noise',

'sta_to_ap_signal', 'sta_to_ap_noise', 'sta_from_ap_signal_max_free_rate',
'sta_from_ap_noise_max_free_rate', 'sta_to_ap_signal_max_free_rate',
'sta_to_ap_noise_max_free_rate', 'sta_from_ap_signal_mean_free_rate',
'sta_from_ap_noise_mean_free_rate', 'sta_to_ap_signal_mean_free_rate',
'sta_to_ap_noise_mean_free_rate',

'sta_to_ap_2_mean_ant_rssi_free_rate', 'sta_to_ap_2_max_ant_rssi_free_rate',
'sta_to_ap_2_sum_ant_rssi_mean',           
'sta_from_ap_2_max_ant_rssi_free_rate',    
'sta_from_ap_2_sum_ant_rssi_mean',         
'sta_from_ap_2_mean_ant_rssi_free_rate',
'seq_time',
]

combined_data2 = combined_data2.drop(columns=columns_to_remove2, errors='ignore')

# # 合并AP2和AP3
# combined_data = pd.concat([combined_data1, combined_data2], axis=1, join='outer')
# combined_data = combined_data.dropna(axis=1, how='any')
combined_data = pd.concat([combined_data1, combined_data2], ignore_index=True)
# 删除 nss 列值为 0 的行
combined_data = combined_data[combined_data['nss'] != 0]

# 创建phy_rate映射字典
phy_rate_map = {
	(0, 1): 8.6,  (0, 2): 17.2,
	(1, 1): 17.2, (1, 2): 34.4,
	(2, 1): 25.8, (2, 2): 51.6,
	(3, 1): 34.4, (3, 2): 68.8,
	(4, 1): 51.6, (4, 2): 103.2,
	(5, 1): 68.8, (5, 2): 137.6,
	(6, 1): 77.4, (6, 2): 154.9,
	(7, 1): 86.0, (7, 2): 172.1,
	(8, 1): 103.2,(8, 2): 206.5,
	(9, 1): 114.7,(9, 2): 229.4,
	(10, 1): 129.0,(10, 2): 258.1,
	(11, 1): 143.4,(11, 2): 286.8
}

# 使用map方法将mcs和nss的组合映射到phy_rate
combined_data['phy_rate'] = combined_data.apply(lambda row: phy_rate_map.get((row['mcs'], row['nss']), None), axis=1)



# 计算相关系数矩阵
correlations = combined_data.corr()

# 获取 'throughput' 与其他变量的相关性并排序
throughput_correlations = correlations['throughput'].sort_values(ascending=False)

print(" 'throughput' 与其他变量的影响强弱排序:")
print(throughput_correlations[1:])

# 选择相关性最强的特征
top_correlated_features = throughput_correlations.index[1:].tolist()


# plt.figure(figsize=(12, 6))

# # 直方图
# sns.histplot(combined_data['throughput'], bins=30, kde=True, color='blue', stat='density', alpha=0.5)

# plt.title('Throughput Distribution')
# plt.xlabel('Throughput')
# plt.ylabel('Density')
# plt.grid(True)
# plt.show()



# 选择相关性最强的特征作为模型输入
X = combined_data[top_correlated_features]
y = combined_data['throughput']

# 分割训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 定义数据预处理器，数值特征标准化
numeric_transformer = StandardScaler()

# 创建预处理步骤
preprocessor = ColumnTransformer(
transformers=[
('num', numeric_transformer, top_correlated_features)
])

# 定义多个模型进行对比
models = {
	'Gradient Boosting': GradientBoostingRegressor(random_state=42),
	'Random Forest': RandomForestRegressor(random_state=42),
	'Support Vector Regressor': SVR(),
	'Linear Regression': LinearRegression()
}

# 初始化一个字典存储模型的性能指标
results = {}

# 遍历每个模型，训练并评估它们的性能
for model_name, model in models.items():
# 创建pipeline
pipeline = Pipeline([
('preprocessor', preprocessor),  # 数据预处理
('regressor', model)  # 使用模型
])

# 训练模型
pipeline.fit(X_train, y_train)

# 预测
y_pred = pipeline.predict(X_test)

# 计算误差和评价指标
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# 将结果存入字典
results[model_name] = {
	'MSE': mse,
	'RMSE': rmse,
	'MAE': mae,
	'R²': r2
}

# 将结果输出为DataFrame
results_df = pd.DataFrame(results).T
print(results_df)

# 使用Stacking集成学习模型
stacking_regressor = StackingRegressor(
estimators=[
('gb', GradientBoostingRegressor(random_state=42)),
('rf', RandomForestRegressor(random_state=42)),
('svr', SVR())
],
final_estimator=LinearRegression()
)

# 创建Stacking模型的pipeline
stacking_pipeline = Pipeline([
('preprocessor', preprocessor),  # 数据预处理
('stacking_regressor', stacking_regressor)  # 集成模型
])

# 训练Stacking模型
stacking_pipeline.fit(X_train, y_train)

# 预测
y_pred_stack = stacking_pipeline.predict(X_test)

# 计算Stacking模型的性能指标
mse_stack = mean_squared_error(y_test, y_pred_stack)
rmse_stack = np.sqrt(mse_stack)
mae_stack = mean_absolute_error(y_test, y_pred_stack)
r2_stack = r2_score(y_test, y_pred_stack)

# 输出Stacking模型的评价指标
print("\nStacking模型性能：")
print(f"MSE: {mse_stack}")
print(f"RMSE: {rmse_stack}")
print(f"MAE: {mae_stack}")
print(f"R²: {r2_stack}")

# 将Stacking模型的性能添加到结果中
results['Stacking Model'] = {
	'MSE': mse_stack,
	'RMSE': rmse_stack,
	'MAE': mae_stack,
	'R²': r2_stack
}

# 更新结果展示
results_df = pd.DataFrame(results).T
print(results_df)

# 参数调整和优化
param_grid = {
	'stacking_regressor__gb__n_estimators': [100, 200],
	'stacking_regressor__gb__learning_rate': [0.05, 0.3],
	'stacking_regressor__rf__n_estimators': [100, 200],
	'stacking_regressor__rf__max_depth': [5, 10],
	'stacking_regressor__svr__C': [0.1, 1, 10],
	'stacking_regressor__svr__kernel': ['linear', 'rbf']
}

grid_search = GridSearchCV(estimator=stacking_pipeline, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)
grid_search.fit(X_train, y_train)
best_params = grid_search.best_params_
best_score = -grid_search.best_score_

print("\nBest Grid Search Score and Parameters:")
print(f"Best Score: {best_score}")
print(f"Best Parameters: {best_params}")

# 最佳参数模型评估
best_stacking_model = grid_search.best_estimator_
y_pred_best_stack = best_stacking_model.predict(X_test)
mse_best_stack = mean_squared_error(y_test, y_pred_best_stack)
rmse_best_stack = np.sqrt(mse_best_stack)
mae_best_stack = mean_absolute_error(y_test, y_pred_best_stack)
r2_best_stack = r2_score(y_test, y_pred_best_stack)

print("\nStacking最佳参数模型性能：")
print("\nOptimized Stacking Model Performance:")
print(f"MSE: {mse_best_stack}")
print(f"RMSE: {rmse_best_stack}")
print(f"MAE: {mae_best_stack}")
print(f"R²: {r2_best_stack}")

\end{lstlisting}





\end{document}
